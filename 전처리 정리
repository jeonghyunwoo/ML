R recipes 패키지의 credit_data를 이용해서 예를 든다.
credit_data는 수치형과 범주형 변수가 섞여있다.
target변수는 status이며 good/bad 값을 가진다.
# 전처리 
(1) data split: xtr, xte, ytr, yte
(2) xtr, xte: 수치형과 범주형을 나눠서 imputing (xtr imputing방법대로)
(3) ytr, yte: bad/good을 0/1로 바꿈
(1) 
from sklearn.model_selection import train_test_split
xtr, xte, ytr, yte = train_test_split(crd.drop('status',axis=1), crd.status, test_size=0.3, random_state=42)

(2)
# imputation과 one hot encoding을 한다
# imputation은 xtr로 fitting한 후 xte에 동일적용한다
# 이를 위해 class를 만든다(imp_ohe)
from sklearn.impute import SimpleImputer
class imp_ohe: 
    def __init__(self, tr):
        self.tr = tr
        self.prep_num = SimpleImputer(strategy='median')
        self.prep_cat = SimpleImputer(strategy='most_frequent')
        self.num_feat = list(tr.select_dtypes(include=np.number).columns)
        self.cat_feat = list(tr.select_dtypes(include='object').columns)
        self.prep_num.fit(self.tr[self.num_feat])
        self.prep_cat.fit(self.tr[self.cat_feat])

    def impute(self, df=None):
        if df is None:
            self.df = self.tr
        else:
            self.df = df
        self.df_n = self.prep_num.transform(self.df[self.num_feat])
        self.df_n = pd.DataFrame(self.df_n,columns = self.num_feat)
        self.df_c = self.prep_cat.transform(self.df[self.cat_feat])
        self.df_c = pd.get_dummies(self.df_c)
        self.df1 = pd.concat([self.df_n,self.df_c],axis=1)
        
        return self.df1

(3) 
le = LabelEncoder()
ytr1 = le.fit_transform(ytr)
yte1 = le.transform(yte)

# 모델링
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier() #결과를 보고 하이퍼파라미터를 파악한다
rf
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
(1) grid search
param_grid = {'bootstrap': [True,False],'n_estimators':[300,500]}
rf_grid = GridSearchCV(rf, param_grid=param_grid, scoring='roc_auc',cv=5,n_jobs=-1)
(2) random search
param_dist = {'bootstrap':[True,False],'n_estimators':[int(x) for x in np.linspace(200,1000,num=10)]}
rf_rand = RandomizedSearchCV(rf, n_iter=10, cv=5, param_distributions=param_dist, scoring='roc_auc',n_jobs=-1)
(3) fitting 
rf_grid.fit(xtr1, ytr1)
rf_rand.fit(xtr1, ytr1)
(4) predict
pred1, pred2 = rf_grid.predict(xte1), rf_rand.predict(xte1)
score1, score2 = rf_grid.predict_proba(xte1)[:,1], rf_rand.predict_proba(xte1)[:,1]
(4) performance 
from sklearn.metrics import accuracy_score,cohen_kappa_score,roc_auc_score,roc_curve,confusion_matrix
print(f'''
accuracy: grid {accuracy_score(yte1,pred1):.3}, rand {accuracy_score(yte1,pred2):.3}
auc      : grid {roc_auc_score(yte1,score1):.3}, rand {roc_auc_score(yte1,score2):.3})
''')

(5) confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt
sns.set(style = 'whitegrid', color_codes=true)
cm = confusion_matrix(yte1, pred1)
sns.heatmap(cm, annot=True, fmt='d',cmap='Reds',xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predict')
plt.ylabel('True')
plt.title('Grid Search - random forest')
plt.tight_layout()

(6) roc curve
fpr, tpr, threshold = roc_curve(yte1, score1)
auc = roc_auc_score(yte1, score1)
plt.plot(fpr, tpr)
plt.annotate(f'auc: {auc:.3}', xy=(0.7,0.3), fontweight='bold', fontsize=14)
